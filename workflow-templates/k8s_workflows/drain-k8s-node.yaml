name: "Cordon Kubernetes node"

on:
  workflow_dispatch:
    inputs:
      clusterName:
        description: "Name of Kubernetes cluster"
        required: true
      nodeName:
        description: "Private DNS name of Kubernetes node to cordon. e.g ip-10-169-153-13.ec2.internal"
        required: true
      deleteLocalData:
        description: "Continue even if there are pods using emptyDir (local data that will be deleted when the node is drained)."
        required: true
        default: false
      dryRun:
        description: "If true, only print the object that would be sent, without sending it."
        required: true
        default: false
      force:
        description: ""Continue even if there are pods not managed by a ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet."
        required: true
        default: false
      gracePeriod:
        description: "Period of time in seconds given to each pod to terminate gracefully. If negative, the default value specified in the pod will be used."
        required: true
        default: -1
      ignoreDaemonsets:
        description: "Ignore DaemonSet-managed pods."
        required: true
        default: true
      timeout:
        description: "The length of time to wait before giving up, zero means infinite."
        required: true
        default: 0

steps:
  drain:
    name: "Drain Kubernetes Node"
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Source Code
        uses: actions/checkout@v2
      - name: "Configure AWS CLI"
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.SRE_EKS_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.SRE_EKS_AWS_ACCESS_KEY_ID }}
          aws-region: "us-east-1"
      - name: "Generate Kubernetes config"
        run: |
          aws eks update-kubeconfig --name ${{ github.event.inputs.clusterName }}
          export KUBE_CONFIG_DATA=$(cat $HOME/.kube/config | base64)
      - name: "Drain Node"
        uses: steebchen/kubectl@master
        with:
          args: "drain ${{ github.event.inputs.nodeName }} --delete-local-data=${{ github.event.inputs.deleteLocalData }} --dry-run=${{ github.event.inputs.dryRun }} --force=${{ github.event.inputs.force }} --grace-period=${{ github.event.inputs.gracePeriod }} --ignore-daemonsets=${{ github.event.inputs.ignoreDaemonsets }} --timeout=${{ github.event.inputs.timeout }}"
      - name: "Drain Failed"
        # https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#example-using-status-check-functions
        if: ${{ failure() }}
        uses: archive/github-actions-slack@v1.0.3
        with:
          slack-bot-user-oauth-access-token: ${{ secrets.SLACK_BOT_USER_OAUTH_ACCESS_TOKEN }}
          slack-channel: terraform
          slack-text: |
            Kubernetes node draining for the node "${{ github.event.inputs.nodeName }}" failed for the "${{ github.event.inputs.clusterName }}" EKS cluster.
      - name: "Drain Was Successful"
        # https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#example-using-status-check-functions
        if: ${{ success() }}
        uses: archive/github-actions-slack@v1.0.3
        with:
          slack-bot-user-oauth-access-token: ${{ secrets.SLACK_BOT_USER_OAUTH_ACCESS_TOKEN }}
          slack-channel: terraform
          slack-text: |
            Kubernetes node draining for the node "${{ github.event.inputs.nodeName }}" was successful for the "${{ github.event.inputs.clusterName }}" EKS cluster.
